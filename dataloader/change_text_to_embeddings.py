import pandas as pd
import numpy as np
import pickle
import os
import json
from transformers import AutoTokenizer, AutoModel
import torch

# -------------------------- 1. é…ç½®è·¯å¾„ï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è°ƒæ•´ï¼‰ --------------------------
EMBEDDING_MODEL = '/roberta'  # ä½ çš„RoBERTaæ¨¡å‹è·¯å¾„
# ç¡®ä¿è¿™ä¸ªCSVæ–‡ä»¶åŒ…å« 'video_path' å’Œ 'per_frame_analysis' è¿™ä¸¤åˆ—
# TEXT_CSV_PATH = 'text_frames_emotiw.csv'
TEXT_CSV_PATH = 'text_frames_daisee.csv'
# æ–°çš„ä¿å­˜è·¯å¾„ï¼Œä»¥åŒºåˆ†æ—§çš„pklæ–‡ä»¶
SAVE_DICT_PATH = 'embedding_dict_timeseries_daisee_16.pkl'


# -------------------------- 2. ä½ çš„JSONè½¬æ¢å‡½æ•° (å·²é›†æˆ) --------------------------
def convert_json_to_sentence_list(json_string):
    """
    å°†åŒ…å«å¸§åˆ†æçš„JSONå­—ç¬¦ä¸²è§£æå¹¶è½¬æ¢ä¸ºå¸¦æœ‰æ—¶é—´æˆ³çš„å¥å­åˆ—è¡¨ã€‚
    æ ¼å¼: "At X.XX seconds, the student's state is: [States]."
    """
    if not isinstance(json_string, str):
        return []
    try:
        list_of_frames = json.loads(json_string)
        sentence_list = []
        
        # ä½¿ç”¨ enumerate è·å–ç´¢å¼• i (å³å…¬å¼ä¸­çš„ n)
        for i, frame_data in enumerate(list_of_frames):
            # è®¡ç®—æ—¶é—´æˆ³: n * 10 / 16
            # å‡è®¾æ¯ä¸ªåˆ‡ç‰‡ä»£è¡¨æ€»æ—¶é•¿10ç§’ä¸­çš„1/16
            timestamp = i * 10.0 / 16.0
            
            # è·å–çŠ¶æ€å†…å®¹ (è¿™é‡Œä¿ç•™äº†ä½ åŸæœ¬åªç”¨ 'Emotional Change' çš„é€»è¾‘)
            # å¦‚æœéœ€è¦æŠŠä¹‹å‰æ³¨é‡Šæ‰çš„ Gaze, Head ç­‰åŠ å›æ¥ï¼Œå¯ä»¥åœ¨è¿™é‡Œå–æ¶ˆæ³¨é‡Šå¹¶æ‹¼æ¥å­—ç¬¦ä¸²
            content = (
                f"{frame_data.get('Gaze Direction', 'N/A')}, "
                f"{frame_data.get('Eyelid State', 'N/A')}, "
                f"{frame_data.get('Head Posture', 'N/A')}, "
                f"{frame_data.get('Mouth Movement', 'N/A')}, "
                f"{frame_data.get('Emotional Change', 'N/A')}"
            )
            
            # æŒ‰ç…§è¦æ±‚æ„å»ºå¥å­
            # ä¾‹å¦‚: "At 0.62 seconds, the student's state is: neutral."
            sentence = f"At {timestamp:.3f} seconds, the student's state is: {content}."
            
            sentence_list.append(sentence)
            
        return sentence_list
    except (json.JSONDecodeError, TypeError) as e:
        print(f"å¤„ç†JSONå­—ç¬¦ä¸²æ—¶å‡ºé”™: {e} | åŸå§‹å­—ç¬¦ä¸²: '{json_string[:100]}...'")
        return []


# -------------------------- 3. åŠ è½½NLPæ¨¡å‹ (æ— éœ€æ”¹åŠ¨) --------------------------
def load_nlp_model(model_name=EMBEDDING_MODEL):
    nlp_tokenizer = None
    nlp_model = None
    try:
        print(f"Loading NLP Embedding Model: {model_name}...")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        nlp_tokenizer = AutoTokenizer.from_pretrained(model_name)
        nlp_model = AutoModel.from_pretrained(model_name).to(device)
        nlp_model.eval()
        print(f"NLP Embedding Model loaded successfully to {device}.")
        return nlp_tokenizer, nlp_model, device
    except Exception as e:
        print(f"Error loading NLP model {model_name}: {e}")
        exit(1)


# -------------------------- 4. è®¡ç®—å¹¶ç”Ÿæˆæ—¶åºåµŒå…¥å­—å…¸ (å·²ä¿®æ”¹) --------------------------
def generate_embedding_dict():
    # åŠ è½½æ¨¡å‹
    tokenizer, model, device = load_nlp_model()
    
    # è¯»å–æ–‡æœ¬CSV
    text_csv_data = pd.read_csv(TEXT_CSV_PATH)
    
    embedding_dict = {}
    
    print("å¼€å§‹ä¸ºæ¯ä¸ªè§†é¢‘ç”Ÿæˆæ—¶åºæ–‡æœ¬åµŒå…¥...")
    # éå†CSVä¸­çš„æ¯ä¸€è¡Œ
    for index, row in text_csv_data.iterrows():
        video_path = row['video_path']
        
        json_string = row['per_frame_analysis'] 
        subject_id = os.path.basename(video_path)

        sentence_list = convert_json_to_sentence_list(json_string)

        if not sentence_list:
            print(f"è­¦å‘Š: å·²è·³è¿‡ {subject_id}ï¼Œå› ä¸ºæœªèƒ½ç”Ÿæˆå¥å­åˆ—è¡¨ã€‚")
            continue

        encoded_input = tokenizer(
            sentence_list,
            padding=True,
            truncation=True,
            max_length=512,  # å¯¹æ¯å¸§ç”Ÿæˆçš„å¥å­é•¿åº¦è¿›è¡Œé™åˆ¶
            return_tensors='pt'
        ).to(device)

        with torch.no_grad():
            model_output = model(**encoded_input)

        time_series_embeddings = model_output.last_hidden_state[:, 0, :].cpu().numpy()

        embedding_dict[subject_id] = time_series_embeddings
        
        # æ‰“å°è¿›åº¦
        if (index + 1) % 50 == 0:
            print(f"å·²å¤„ç† {index + 1}/{len(text_csv_data)} ä¸ªè§†é¢‘...")

    with open(SAVE_DICT_PATH, 'wb') as f:
        pickle.dump(embedding_dict, f)
        
    print(f"\nâœ… æ—¶åºæ–‡æœ¬åµŒå…¥å­—å…¸å·²æˆåŠŸä¿å­˜åˆ°ï¼š{SAVE_DICT_PATH}")
    print(f"ğŸ“Š å­—å…¸åŒ…å« {len(embedding_dict)} ä¸ªè§†é¢‘çš„åµŒå…¥")

    # éªŒè¯ä¸€ä¸‹ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å½¢çŠ¶æ˜¯å¦æ­£ç¡®
    if embedding_dict:
        first_key = list(embedding_dict.keys())[-1]
        print(f"ç¤ºä¾‹: '{first_key}' çš„åµŒå…¥å½¢çŠ¶ä¸º: {embedding_dict[first_key].shape}")


if __name__ == "__main__":
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    save_dir = os.path.dirname(SAVE_DICT_PATH)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    # ç”Ÿæˆå¹¶ä¿å­˜å­—å…¸
    generate_embedding_dict()